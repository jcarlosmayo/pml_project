---
title: "Coursera - Practical Machine Learning"
output: html_document
---

## Project description
In this project, your goal will be to use data from accelerometers on the belt, forearm, arm, and dumbell of 6 participants. They were asked to perform barbell lifts correctly and incorrectly in 5 different ways. More information is available from the website [here](http://groupware.les.inf.puc-rio.br/har) under the section on the Weight Lifting Exercise Dataset. 

The goal of your project is to predict the manner in which they did the exercise. This is the "classe" variable in the training set. You may use any of the other variables to predict with. You should create a report describing how you built your model, how you used cross validation, what you think the expected out of sample error is, and why you made the choices you did. You will also use your prediction model to predict 20 different test cases. 

1. Your submission should consist of a link to a Github repo with your R markdown and compiled HTML file describing your analysis. Please constrain the text of the writeup to < 2000 words and the number of figures to be less than 5. It will make it easier for the graders if you submit a repo with a gh-pages branch so the HTML page can be viewed online (and you always want to make it easy on graders :-).
2. You should also apply your machine learning algorithm to the 20 test cases available in the test data above. Please submit your predictions in appropriate format to the programming assignment for automated grading. See the programming assignment for additional details.

```{r read training data}
library(caret); library(ggplot2)
train <- read.csv("data/pml-training.csv")
```

## Cleaning data

### NAs
```{r NA values}
# Emtpy vectors where to collect the results
na_var_id <- c()
na_var_name <- c()
na_percentage <- c()

for (i in 1:ncol(train)){
        # Check NA values for each of the variables
        na_avg <- mean(is.na(train[,i]))
        
        # If the NA average is larger than 0 collect it in the data frame
        if (na_avg > 0){
                na_var_id <- c(na_var_id, i)
                na_var_name <- c(na_var_name, names(train)[i])
                na_percentage <- c(na_percentage, round(na_avg, 2))
        }
}

# Merge into a data frame
na_values <- as.data.frame(cbind(na_var_id, na_var_name, na_percentage))
# Assign correct data class to each column
na_values[,1] <- as.integer(as.character(na_values[,1]))
na_values[,2] <- as.character(na_values[,2])
na_values[,3] <- as.numeric(as.character(na_values[,3]))
rm(i, na_avg, na_var_id, na_var_name, na_percentage)
```
67 variables display a very large amount of NA observations. Approximately 98% of the observations in each of those variables. It would be possible to impute them using the existing values but it seems far-fetched to recreate 98% of the data using the existing 2%. Ideally, each of the affected variables should be inspected to determine whether imputation is reasonable or not. However, for the purpose of this project, since imputing the missing values could introduce bias that would affect 98% of 67 variables in a set of 159, which amounts to approximately 41% of the total data, I will simply remove them from the training data.
```{r remove 98% na variables from train data}
clean_train <- train[,-na_values$na_var_id]
```

### Factor variables

On the resulting set there are 36 factor variables that have to be inspected to see whether they are actual factor variablesor not and they could be converted to a numeric value. And within the latter, which are meaningful to our analysis. The first ones, variables 2, 5 and 6, can be excluded from this analysis as they correspond to subject's name, time stamp, and whether there was a new window or not.
```{r factor variables}
# Empty vector to collect the id of factor variables
factor_var <- c()
count <- 0
for (i in 7:(ncol(clean_train)-1)){
        if (class(clean_train[,i])=="factor"){
                count <- count + 1
                factor_var <- c(factor_var, i)
        }
}
count
rm(i, count)
```
For the remaining 33 variables, as they all seem to have been mistakenly entered as factors even though they contain numeric values, I will convert them to numeric values.

```{r cleaning factor variables}
############################################
# Convert factors values to numeric values #
############################################

for (i in 1:length(factor_var)){
        x <- clean_train[, factor_var[i]]
        x <- as.numeric(levels(x))[x]
        clean_train[, factor_var[i]] <- x
}

rm(x)

###################################################################
# Check the amount of NA values per variable after the conversion #
###################################################################

na_factor_var_id <- c()
na_factor_var_name <- c()
na_factor_percentage <- c()

for (i in 1:ncol(clean_train)){
        # Check NA values for each of the variables
        na_avg <- mean(is.na(clean_train[,i]))
        
        # If the NA average is larger than 0 collect it in the data frame
        if (na_avg > 0){
                na_factor_var_id <- c(na_factor_var_id, i)
                na_factor_var_name <- c(na_factor_var_name, names(clean_train)[i])
                na_factor_percentage <- c(na_factor_percentage, round(na_avg, 2))
        }
}

# Merge into a data frame
na_factor_values <- as.data.frame(cbind(na_factor_var_id, na_factor_var_name, na_factor_percentage))
# Assign correct data class to each column
na_factor_values[,1] <- as.integer(as.character(na_factor_values[,1]))
na_factor_values[,2] <- as.character(na_factor_values[,2])
na_factor_values[,3] <- as.numeric(as.character(na_factor_values[,3]))
rm(i, na_avg, na_factor_var_id, na_factor_var_name, na_factor_percentage)
```


```{r remove 98% na factor variables from train data}
clean_train <- clean_train[,-na_factor_values$na_factor_var_id]
```

## Split the train data into separate ones

## Correlated predictors and PCA

We will leave out the last column in the data set, which is the outcome, 'classe'. We can then calculate the absolute value of the correlation between all the other variables, remove the correlation of a variable with itself, which will be the value displayed in the diagonal, and inspect the variables that show a correlation higher than 0.8.
```{r correlation and PCA}
# Calculate the absolute correlation among all variables, except the outcome
#cor_table <- abs(cor(train[,-160]))
```

