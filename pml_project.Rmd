---
title: "Practical Machine Learning"
output: html_document
---

## 1 - Description
Six participants were asked to perform barbell lifts correctly and incorrectly in 5 different ways while accelerometers on the belt, forearm, arm, and dumbell measured their performance. The original study and data can be found [here](http://groupware.les.inf.puc-rio.br/har) under the section on the Weight Lifting Exercise Dataset.

The goal of this project is to predict the manner in which they did the exercise. This is reflected in the "classe" variable, which can be either of five qualitative values: A, B, C, D or E.

## 2 - Data Cleaning

Including the outcome variable, the data set contains 11,776 observations and 160 variables. However, a quick exploration of the data shows that there is a huge amount of missing values and that they are concentrated in a number of variables. No explanation is provided about why there are so many missing values although in fact they affect a large percentage of the data set. In 73 of the variables 98% of the observations are missing values, and in another 27 they reach 100%.
  
It would be possible to impute them using the existing values but it seems far-fetched to recreate 98% of the data using the existing 2%. Ideally, each of the affected variables should be inspected to determine whether imputation is reasonable or not. However, for the purpose of this project, I will simply remove them from the training data.

Additionally I will remove variables that are not relevant to the model, and which relate to observation's id, subject's name, time stamps and observation windows. These appear to track other values not directly related with the performance of the exercises and which would be meaningless once the model constructed is fit to new data.

Therefore, including the outcome variable, there remain 53 variables to train a machine learning model (see fig.1).

## 3 - Model selection
Before training different models to fit the data the clean training data was split into a training set, a testing set and a validation set, corresponding respectively to 60, 20 and 20% of the data.

### 3.1 - Model training
Initially a classification tree model was fit to the training set, however, the results were very poor. The in-sample accuracy was 49.91%, and , logically, it performed equally poor on the testing and validation sets with a 49.38% and 48.69% accuracay respectively.

The next model fitted was a random forest with all variables, 52 plus the outcome, using the default settings, which uses bootstrapping with 25 repetitions and generates 500 trees. It provided an impressing 0.87% OOB error estimate. The downside was the relatively long time to perform the calculations. In contrast a 4-fold cross validation model using the same number of trees is indeed much faster and the results quite similar. The OOB was 0.94%.

One way to increase the efficiency of the model is to inspect the relative importance of the variables used. Since not all variables are equally significant it should be possible to reduce the number of variables without losing predictive accuracy.

```{r variable importance, echo=FALSE, fig.align='center', fig.height=10, fig.width=10}
library(ggplot2)
ggplot(sort_importance_asc, aes(x=var, y=MeanDecreaseGini)) +
        geom_bar(stat="identity", fill="red", colour="white") +
        coord_flip() +
        xlab("Variables") +
        ylab("Mean Decrease Gini") +
        ggtitle("figure 1 - Variable importance")
```

### 3.2 Model comparison

Although there is a downward trend in accuracy as the number of variables decreases, it is outstanding how each subsequent model using just half of the variables used in the preceding one display just as much accuracy. It is also noteworthy how a 5-fold cross validation model results in as much predicting power as a bootstrap model but requires much less computing power, which makes it much more efficient.  
  
**Figure 2 - Comparison table**  
<style type="text/css">
.tg  {border-collapse:collapse;border-spacing:0;border:none;margin:0px auto;}
.tg td{font-family:Arial, sans-serif;font-size:14px;padding:10px 5px;border-style:solid;border-width:0px;overflow:hidden;word-break:normal;}
.tg th{font-family:Arial, sans-serif;font-size:14px;font-weight:normal;padding:10px 5px;border-style:solid;border-width:0px;overflow:hidden;word-break:normal;}
.tg .tg-3zgm{background-color:#343434;color:#ffffff}
.tg .tg-aq88{background-color:#343434;color:#ffffff;text-align:center}
.tg .tg-s6z2{text-align:center}
.tg .tg-0ord{text-align:right}
.tg .tg-2thk{background-color:#c0c0c0;text-align:center}
.tg .tg-tkkh{background-color:#c0c0c0;text-align:right}
.tg .tg-hy62{background-color:#c0c0c0}
</style>
<table class="tg">
  <tr>
    <th class="tg-3zgm"></th>
    <th class="tg-3zgm"></th>
    <th class="tg-aq88">Error estimate %<br>OOB<br></th>
    <th class="tg-aq88">Accuracy %<br>Testing<br></th>
    <th class="tg-aq88">Accuracy %<br>Validation</th>
  </tr>
  <tr>
    <td class="tg-s6z2">52 variables<br></td>
    <td class="tg-0ord">Bootstrap<br></td>
    <td class="tg-s6z2">0.87</td>
    <td class="tg-s6z2">98.94</td>
    <td class="tg-s6z2">99.08</td>
  </tr>
  <tr>
    <td class="tg-s6z2"></td>
    <td class="tg-0ord">5-fold CV<br></td>
    <td class="tg-s6z2">0.94</td>
    <td class="tg-s6z2">99.26</td>
    <td class="tg-s6z2">99.03</td>
  </tr>
  <tr>
    <td class="tg-2thk">26 variables<br></td>
    <td class="tg-tkkh">Bootstrap<br></td>
    <td class="tg-2thk">0.83</td>
    <td class="tg-2thk">99.18</td>
    <td class="tg-2thk">98.93</td>
  </tr>
  <tr>
    <td class="tg-2thk"></td>
    <td class="tg-tkkh">5-fold CV<br></td>
    <td class="tg-2thk">0.82</td>
    <td class="tg-2thk">99.31<br></td>
    <td class="tg-2thk">98.98</td>
  </tr>
  <tr>
    <td class="tg-s6z2">13 variables<br></td>
    <td class="tg-0ord">Bootstrap<br></td>
    <td class="tg-s6z2">1.74</td>
    <td class="tg-s6z2">98.37</td>
    <td class="tg-s6z2">98.57</td>
  </tr>
  <tr>
    <td class="tg-s6z2"><br></td>
    <td class="tg-0ord">5-fold CV<br></td>
    <td class="tg-s6z2">1.65<br></td>
    <td class="tg-s6z2">98.39</td>
    <td class="tg-s6z2">98.27</td>
  </tr>
  <tr>
    <td class="tg-2thk">7 variables<br></td>
    <td class="tg-tkkh">Bootstrap<br></td>
    <td class="tg-2thk">1.92<br></td>
    <td class="tg-2thk">98.52</td>
    <td class="tg-2thk">98.78</td>
  </tr>
  <tr>
    <td class="tg-hy62"></td>
    <td class="tg-tkkh">5-fold CV<br></td>
    <td class="tg-2thk">1.86</td>
    <td class="tg-2thk">98.39</td>
    <td class="tg-2thk">98.75</td>
  </tr>
</table>

### 4 Final model
Using the 7 most important vsariables and fitting a 5-fold cross validation model yields excellent results in terms of OOB error estimate and final predicting accuracy, while being very efficient in terms of computing power.

        fitControl <- trainControl(method="cv", number=5)
        
        final_model <- train(classe ~ roll_belt + pitch_forearm + yaw_belt + magnet_dumbbell_y +
                                pitch_belt + magnet_dumbbell_z + roll_forearm, 
                                data = training,
                                trControl = fitControl)

