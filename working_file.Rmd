---
title: "Coursera - Practical Machine Learning"
output: html_document
---

## Project description
In this project, your goal will be to use data from accelerometers on the belt, forearm, arm, and dumbell of 6 participants. They were asked to perform barbell lifts correctly and incorrectly in 5 different ways. More information is available from the website [here](http://groupware.les.inf.puc-rio.br/har) under the section on the Weight Lifting Exercise Dataset. 

The goal of your project is to predict the manner in which they did the exercise. This is the "classe" variable in the training set. You may use any of the other variables to predict with. You should create a report describing how you built your model, how you used cross validation, what you think the expected out of sample error is, and why you made the choices you did. You will also use your prediction model to predict 20 different test cases. 

1. Your submission should consist of a link to a Github repo with your R markdown and compiled HTML file describing your analysis. Please constrain the text of the writeup to < 2000 words and the number of figures to be less than 5. It will make it easier for the graders if you submit a repo with a gh-pages branch so the HTML page can be viewed online (and you always want to make it easy on graders :-).
2. You should also apply your machine learning algorithm to the 20 test cases available in the test data above. Please submit your predictions in appropriate format to the programming assignment for automated grading. See the programming assignment for additional details.

```{r read training data}
library(caret); library(ggplot2)
raw_data <- read.csv("data/pml-training.csv")
```

## Cleaning data

### NAs
```{r NA values}
# Emtpy vectors where to collect the results
na_var_id <- c()
na_var_name <- c()
na_percentage <- c()

for (i in 1:ncol(raw_data)){
        # Check NA values for each of the variables
        na_avg <- mean(is.na(raw_data[,i]))
        
        # If the NA average is larger than 0 collect it in the data frame
        if (na_avg > 0){
                na_var_id <- c(na_var_id, i)
                na_var_name <- c(na_var_name, names(raw_data)[i])
                na_percentage <- c(na_percentage, round(na_avg, 2))
        }
}

# Merge into a data frame
na_values <- as.data.frame(cbind(na_var_id, na_var_name, na_percentage))
# Assign correct data class to each column
na_values[,1] <- as.integer(as.character(na_values[,1]))
na_values[,2] <- as.character(na_values[,2])
na_values[,3] <- as.numeric(as.character(na_values[,3]))
rm(i, na_avg, na_var_id, na_var_name, na_percentage)
```
67 variables display a very large amount of NA observations. Approximately 98% of the observations in each of those variables. It would be possible to impute them using the existing values but it seems far-fetched to recreate 98% of the data using the existing 2%. Ideally, each of the affected variables should be inspected to determine whether imputation is reasonable or not. However, for the purpose of this project, since imputing the missing values could introduce bias that would affect 98% of 67 variables in a set of 159, which amounts to approximately 41% of the total data, I will simply remove them from the training data.
```{r remove 98% na variables from train data}
clean_data <- raw_data[,-na_values$na_var_id]
```

### Factor variables

On the resulting set there are 36 factor variables that have to be inspected to see whether they are actual factor variablesor not and they could be converted to a numeric value. And within the latter, which are meaningful to our analysis. The first ones, variables 2, 5 and 6, can be excluded from this analysis as they correspond to subject's name, time stamp, and whether there was a new window or not.
```{r factor variables}
# Empty vector to collect the id of factor variables
factor_var <- c()
count <- 0
for (i in 7:(ncol(clean_data)-1)){
        if (class(clean_data[,i])=="factor"){
                count <- count + 1
                factor_var <- c(factor_var, i)
        }
}
count
rm(i, count)
```
For the remaining 33 variables, as they all seem to have been mistakenly entered as factors even though they contain numeric values, I will convert them to numeric values.

```{r cleaning factor variables, message=FALSE, warning=FALSE}
############################################
# Convert factors values to numeric values #
############################################

for (i in 1:length(factor_var)){
        x <- clean_data[, factor_var[i]]
        x <- as.numeric(levels(x))[x]
        clean_data[, factor_var[i]] <- x
}

rm(x)

###################################################################
# Check the amount of NA values per variable after the conversion #
###################################################################

na_factor_var_id <- c()
na_factor_var_name <- c()
na_factor_percentage <- c()

for (i in 1:ncol(clean_data)){
        # Check NA values for each of the variables
        na_avg <- mean(is.na(clean_data[,i]))
        
        # If the NA average is larger than 0 collect it in the data frame
        if (na_avg > 0){
                na_factor_var_id <- c(na_factor_var_id, i)
                na_factor_var_name <- c(na_factor_var_name, names(clean_data)[i])
                na_factor_percentage <- c(na_factor_percentage, round(na_avg, 2))
        }
}

# Merge into a data frame
na_factor_values <- as.data.frame(cbind(na_factor_var_id, na_factor_var_name, na_factor_percentage))
# Assign correct data class to each column
na_factor_values[,1] <- as.integer(as.character(na_factor_values[,1]))
na_factor_values[,2] <- as.character(na_factor_values[,2])
na_factor_values[,3] <- as.numeric(as.character(na_factor_values[,3]))
rm(i, na_avg, na_factor_var_id, na_factor_var_name, na_factor_percentage)
```


```{r remove 98% na factor variables from train data}
clean_data <- clean_data[,-na_factor_values$na_factor_var_id]
```

Additionally I will remove variables that are not relevant to the model, which are variables 1 to 7. They relate to observation's id, subject's name, time stamps and seemingly some observation window. These appear to track other values not directly related with the performance of the exercises.
```{r non-relevant variables}
names(clean_data)[1:7]
clean_data <- clean_data[,-c(1:7)]
```

## Split data into train, testing and validation
Before fitting a model I will split the data into three different sets, training, testing and validation, that will represent respectively 60%, 20% and 20% of the data.
```{r split data}
# Split the data into 60-40
train <- createDataPartition(clean_data$classe,
                             p = 0.6, list = FALSE)

# Training set
training <- clean_data[train,]

# Create testing and validation sets
test_valid <- clean_data[-train,]
# Split the testing set into 50-50
test <- createDataPartition(test_valid$classe,
                            p = 0.5, list = FALSE)
# Create the final testing set
testing <- test_valid[test,]
# Validation set
validation <- test_valid[-test,]

# nrow(training) / nrow(clean_data)
# nrow(testing) / nrow(clean_data)
# nrow(validation) / nrow(clean_data)     
```

## Prediction models

### Tree
```{r tree}
# Create the model
tree_fit <- train(classe ~ . , method="rpart", data = training)

# In-sample Confusion matrix
confusionMatrix(predict(tree_fit, training), # Rows
      training$classe) # Columns
```

### Random forest
```{r random forest, cache = TRUE}
set.seed(2110)

################
# Bootstrapped #
################

rf_fit <- train(classe ~ . , method="rf", data = training)

# In-sample Confusion matrix
confusionMatrix(predict(rf_fit, training),
      training$classe)


###########################
# 5-fold cross validation #
###########################
fitControl <- trainControl(method="cv", number=5)
rf_cv_fit <- train(classe ~ . , method="rf", data = training, trControl = fitControl)

# In-sample Confusion matrix
confusionMatrix(predict(rf_cv_fit, training),
      training$classe)
```

```{r variable importance, fig.align = 'center'}
importance <- as.data.frame(rf_fit$finalModel$importance)
importance$var <- row.names(importance)
library(dplyr)
sort_importance <- arrange(importance, desc(MeanDecreaseGini))

sort_importance$var <- factor(sort_importance$var,
                              levels = sort_importance$var)

# Take the 50% most important variables -> 26 variables
var_26_imp <- as.character(sort_importance$var)[1:26]

# Take the 25% most important variables -> 13 variables
var_13_imp <- as.character(sort_importance$var)[1:13]

# Take the 12.5% most important variables -> 7
var_7_imp <- as.character(sort_importance$var)[1:7]

# Take the 4 most important variables -> 4
var_4_imp <- as.character(sort_importance$var)[1:4]
```

```{r importance plot}
sort_importance_asc <- arrange(importance, MeanDecreaseGini)

sort_importance_asc$var <- factor(sort_importance_asc$var,
                              levels = sort_importance_asc$var)

ggplot(sort_importance_asc, aes(x = var, y=MeanDecreaseGini)) +
        geom_point() +
        coord_flip()

ggplot(sort_importance_asc, aes(x=var, y=MeanDecreaseGini)) +
        geom_bar(stat="identity", fill="red", colour="white") +
        coord_flip()
```

```{r random forest 26 variables, cache=TRUE}
################
# Bootstrapped #
################
training_26 <- training[,c(var_26_imp, "classe")]
rf_26_fit <- train(classe ~ . , data=training_26, method = "rf")

# Confusion matrix
confusionMatrix(predict(rf_26_fit, training_26),
      training_26$classe)


###########################
# 5-fold cross validation #
###########################
rf_26_cv_fit <- train(classe ~ . , data=training_26, method = "rf", trControl = fitControl)

# Confusion matrix
confusionMatrix(predict(rf_26_cv_fit, training_26),
      training_26$classe)
```

```{r random forest 13 variables, cache=TRUE}
################
# Bootstrapped #
################
training_13 <- training[,c(var_13_imp, "classe")]
rf_13_fit <- train(classe ~ . , data=training_13, method = "rf")

# Confusion matrix
confusionMatrix(predict(rf_13_fit, training_13),
      training_13$classe)


###########################
# 5-fold cross validation #
###########################
rf_13_cv_fit <- train(classe ~ . , data=training_13, method = "rf", trControl = fitControl)

# Confusion matrix
confusionMatrix(predict(rf_13_cv_fit, training_13),
      training_13$classe)
```

```{r random forest 7 variables, cache=TRUE}
################
# Bootstrapped #
################
training_7 <- training[,c(var_7_imp, "classe")]
rf_7_fit <- train(classe ~ . , data=training_7, method = "rf")


# Confusion matrix
confusionMatrix(predict(rf_7_fit, training_7),
                training_7$classe)


###########################
# 5-fold cross validation #
###########################
rf_7_cv_fit <- train(classe ~ . , data=training_7, method = "rf", trControl = fitControl)

# Confusion matrix
confusionMatrix(predict(rf_7_cv_fit, training_7),
                training_7$classe)
```

```{r random forest 4 variables, cache=TRUE}
################
# Bootstrapped #
################
training_4 <- training[,c(var_4_imp, "classe")]
rf_4_fit <- train(classe ~ . , data=training_4, method = "rf")


# Confusion matrix
confusionMatrix(predict(rf_4_fit, training_4),
                training_4$classe)


###########################
# 5-fold cross validation #
###########################
rf_4_cv_fit <- train(classe ~ . , data=training_4, method = "rf", trControl = fitControl)

# Confusion matrix
confusionMatrix(predict(rf_4_cv_fit, training_4),
                training_4$classe)
```


## Testing the models
Tree model
```{r tree model test}
confusionMatrix(predict(tree_fit, testing),
                testing$classe)
```

Random forest with all valid variables, 53
```{r test 53 variables}
# Bootstrap
confusionMatrix(predict(rf_fit, testing),
                testing$classe)

# 5-fold cross validation
confusionMatrix(predict(rf_cv_fit, testing),
                testing$classe)
```

Random forest with 50% of the most important variables, 26
```{r test 26 variables}
testing_26 <- testing[,c(var_26_imp, "classe")]

# Bootstrapped
confusionMatrix(predict(rf_26_fit, testing_26),
                testing_26$classe)

# 5-fold cross validation
confusionMatrix(predict(rf_26_cv_fit, testing_26),
                testing_26$classe)
```

Random forest with 25% of the most important variables, 13
```{r test 13 variables}
testing_13 <- testing[,c(var_13_imp, "classe")]

# Bootstrapped
confusionMatrix(predict(rf_13_fit, testing_13),
                testing_13$classe)

# 5-fold cross validation
confusionMatrix(predict(rf_13_cv_fit, testing_13),
                testing_13$classe)
```

Random forest with 12.5% of the most important variables, 7
```{r test 7 variables}
testing_7 <- testing[,c(var_7_imp, "classe")]

# Bootstrapped
confusionMatrix(predict(rf_7_fit, testing_7),
                testing_7$classe)

# 5-fold cross validation
confusionMatrix(predict(rf_7_cv_fit, testing_7),
                testing_7$classe)
```

```{r test 4 variables}
testing_4 <- testing[,c(var_4_imp, "classe")]

# Bootstrapped
confusionMatrix(predict(rf_4_fit, testing_4),
                testing_4$classe)

# 5-fold cross validation
confusionMatrix(predict(rf_4_cv_fit, testing_4),
                testing_4$classe)
```

## Validation
Tree model
```{r tree model test}
confusionMatrix(predict(tree_fit, validation),
                validation$classe)
```

Random forest with all valid variables, 53
```{r test 53 variables}
# Bootstrap
confusionMatrix(predict(rf_fit, validation),
                validation$classe)

# 5-fold cross validation
confusionMatrix(predict(rf_cv_fit, validation),
                validation$classe)
```

Random forest with 50% of the most important variables, 26
```{r test 26 variables}
validation_26 <- validation[,c(var_26_imp, "classe")]

# Bootstrapped
confusionMatrix(predict(rf_26_fit, validation_26),
                validation_26$classe)

# 5-fold cross validation
confusionMatrix(predict(rf_26_cv_fit, validation_26),
                validation_26$classe)
```

Random forest with 25% of the most important variables, 13
```{r test 13 variables}
validation_13 <- validation[,c(var_13_imp, "classe")]

# Bootstrapped
confusionMatrix(predict(rf_13_fit, validation_13),
                validation_13$classe)

# 5-fold cross validation
confusionMatrix(predict(rf_13_cv_fit, validation_13),
                validation_13$classe)
```

Random forest with 12.5% of the most important variables, 7
```{r test 7 variables}
validation_7 <- validation[,c(var_7_imp, "classe")]

# Bootstrapped
confusionMatrix(predict(rf_7_fit, validation_7),
                validation_7$classe)

# 5-fold cross validation
confusionMatrix(predict(rf_7_cv_fit, validation_7),
                validation_7$classe)
```

```{r test 4 variables}
validation_4 <- validation[,c(var_4_imp, "classe")]

# Bootstrapped
confusionMatrix(predict(rf_4_fit, validation_4),
                validation_4$classe)

# 5-fold cross validation
confusionMatrix(predict(rf_4_cv_fit, validation_4),
                validation_4$classe)
```